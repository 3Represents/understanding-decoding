_target_: pytorch_lightning.Trainer

# set `1` to train on GPU, `0` to train on CPU only
gpus: 1

accumulate_grad_batches: 1

max_steps: 2000

# val_check_interval can be a float 0.5 (2 times per training epoch) or an integer 1000 (every 1000 training steps)
# Currently, if you use int for val_check_interval, it has to be smaller than the number of batches per epoch, but it is being addressed here: https://github.com/PyTorchLightning/pytorch-lightning/pull/11993
# ToDo: Verify that it works once the update lands
val_check_interval: 0.5 # 1000
#val_check_interval: ${mult_int:${.accumulate_grad_batches}, 1000}

weights_summary: null
progress_bar_refresh_rate: 5
resume_from_checkpoint: null

gradient_clip_val: 0.1
gradient_clip_algorithm: "norm"
#log_every_n_steps: ${mult_int:${.accumulate_grad_batches}, 50} # log every 50 optimizer steps
