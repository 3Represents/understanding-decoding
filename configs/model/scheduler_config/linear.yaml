scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 1
  num_training_steps: 200

scheduler_dict:
  scheduler: null # the schedule instance defined above â€“ will be passed from the code (in configure_optimizer)
  interval: "epoch" # The unit of the scheduler's step size. 'step' or 'epoch
  frequency: 1 # corresponds to updating the learning rate after every `frequency` epoch/step
  name: LearningRateScheduler_${model.scheduler_config.scheduler._target_}
    }
