scheduler:
  _target_: transformers.get_polynomial_decay_schedule_with_warmup
  num_warmup_steps: 500
  num_training_steps: 200000
  lr_end: 0.0

scheduler_dict:
  scheduler: null # the schedule instance defined above â€“ will be passed from the code (in configure_optimizer)
  interval: "step" # The unit of the scheduler's step size. 'step' or 'epoch
  frequency: 1 # corresponds to updating the learning rate after every `frequency` epoch/step
  name: LearningRateScheduler_${model.scheduler_config.scheduler._target_}
    }
